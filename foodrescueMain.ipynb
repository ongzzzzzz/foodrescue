{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "foodrescueMain.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPOQJqH/rUInPbaFejTSOmE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fogeinator/foodrescue/blob/master/foodrescueMain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zFpOZX0PGcI"
      },
      "source": [
        "## Getting the Dataset\r\n",
        "\r\n",
        "https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn5sKv-APTlV"
      },
      "source": [
        "!pip install --quiet kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyij1-_zPwLN"
      },
      "source": [
        "# Choose kaggle.json that created for new API token in your account\r\n",
        "from google.colab import files\r\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjbLrsCOPyVK"
      },
      "source": [
        "!mkdir -p ~/.kaggle\r\n",
        "!cp kaggle.json ~/.kaggle/\r\n",
        "# Check the file in its new directory\r\n",
        "!ls /root/.kaggle/\r\n",
        "# Check the file permission\r\n",
        "!ls -l ~/.kaggle/kaggle.json\r\n",
        "# Change the file permission\r\n",
        "# chmod 600 file ‚Äì owner can read and write | chmod 700 file ‚Äì owner can read, write and execute\r\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Irf0jwP5UJ"
      },
      "source": [
        "# Get the dataset we want\n",
        "# CAUTION: IT IS 4 MF GB OF DATA\n",
        "!kaggle datasets download -d sriramr/fruits-fresh-and-rotten-for-classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsiiCpr_RZlt"
      },
      "source": [
        "# Unzip the dataset\r\n",
        "!mkdir data\r\n",
        "!unzip fruits-fresh-and-rotten-for-classification.zip -d data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm9Yq61wd4-w"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFrc7IJaRoRE"
      },
      "source": [
        "%matplotlib inline\r\n",
        "%config InlineBackend.figure_format = 'retina'\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F\r\n",
        "from torchvision import datasets, transforms, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83MUD8AHRyrG"
      },
      "source": [
        "data_dir = 'data/dataset/train'\r\n",
        "\r\n",
        "# split train test data for training\r\n",
        "def load_split_train_test(datadir, valid_size = .2):\r\n",
        "    # transform images\r\n",
        "    train_transforms = transforms.Compose([transforms.Resize((224, 224)),\r\n",
        "                                       transforms.ToTensor(),\r\n",
        "                                       ])\r\n",
        "    test_transforms = transforms.Compose([transforms.Resize((224, 224)),\r\n",
        "                                      transforms.ToTensor(),\r\n",
        "                                      ])\r\n",
        "    train_data = datasets.ImageFolder(datadir,       \r\n",
        "                    transform=train_transforms)\r\n",
        "    test_data = datasets.ImageFolder(datadir,\r\n",
        "                    transform=test_transforms)\r\n",
        "    \r\n",
        "    num_train = len(train_data)\r\n",
        "    indices = list(range(num_train))\r\n",
        "    split = int(np.floor(valid_size * num_train))\r\n",
        "    np.random.shuffle(indices)\r\n",
        "\r\n",
        "    from torch.utils.data.sampler import SubsetRandomSampler\r\n",
        "\r\n",
        "    train_idx, test_idx = indices[split:], indices[:split]\r\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\r\n",
        "    test_sampler = SubsetRandomSampler(test_idx)\r\n",
        "    trainloader = torch.utils.data.DataLoader(train_data,\r\n",
        "                   sampler=train_sampler, batch_size=64)\r\n",
        "    testloader = torch.utils.data.DataLoader(test_data,\r\n",
        "                   sampler=test_sampler, batch_size=64)\r\n",
        "    \r\n",
        "    return trainloader, testloader\r\n",
        "\r\n",
        "    \r\n",
        "trainloader, testloader = load_split_train_test(data_dir, .2)\r\n",
        "print(trainloader.dataset.classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bfmqF5eR3Fc"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "model = models.resnet50(pretrained=True)\r\n",
        "# print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5CQ6b6gSFAz"
      },
      "source": [
        "for param in model.parameters():\r\n",
        "    param.requires_grad = False\r\n",
        "    \r\n",
        "# fully connected layer in NN\r\n",
        "model.fc = nn.Sequential(nn.Linear(2048, 512),\r\n",
        "                         nn.ReLU(),\r\n",
        "                         nn.Dropout(0.2),\r\n",
        "                         nn.Linear(512, 10),\r\n",
        "                         nn.LogSoftmax(dim=1))\r\n",
        "criterion = nn.NLLLoss()\r\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\r\n",
        "model.to(device)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtU3UBlTSKH1"
      },
      "source": [
        "# actual model training\r\n",
        "# note: use GPU or else you will die of waiting with CPU\r\n",
        "\r\n",
        "# 2 is more than enough: see why in the graph below\r\n",
        "epochs = 2\r\n",
        "steps = 0\r\n",
        "running_loss = 0\r\n",
        "print_every = 10\r\n",
        "train_losses, test_losses = [], []\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "    for inputs, labels in trainloader:\r\n",
        "        steps += 1\r\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        logps = model.forward(inputs)\r\n",
        "        loss = criterion(logps, labels)\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        running_loss += loss.item()\r\n",
        "        \r\n",
        "        if steps % print_every == 0:\r\n",
        "            test_loss = 0\r\n",
        "            accuracy = 0\r\n",
        "            model.eval()\r\n",
        "            \r\n",
        "            with torch.no_grad():\r\n",
        "                for inputs, labels in testloader:\r\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "                    logps = model.forward(inputs)\r\n",
        "\r\n",
        "                    batch_loss = criterion(logps, labels)\r\n",
        "                    test_loss += batch_loss.item()\r\n",
        "                    \r\n",
        "                    ps = torch.exp(logps)\r\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\r\n",
        "                    equals = top_class == labels.view(*top_class.shape)\r\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\r\n",
        "\r\n",
        "            train_losses.append(running_loss/len(trainloader))\r\n",
        "            test_losses.append(test_loss/len(testloader))                    \r\n",
        "            print(f\"Epoch {epoch+1}/{epochs}.. \"\r\n",
        "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\r\n",
        "                  f\"Test loss: {test_loss/len(testloader):.3f}.. \"\r\n",
        "                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\r\n",
        "            running_loss = 0\r\n",
        "            model.train()\r\n",
        "torch.save(model, 'fruitmodel.pth')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bamtxI6pSO6w"
      },
      "source": [
        "# plot of training and testing losses\r\n",
        "matplotlib.rc('xtick',color='w')\r\n",
        "matplotlib.rc('ytick',color='w')\r\n",
        "\r\n",
        "plt.plot(train_losses, label='Training loss')\r\n",
        "plt.plot(test_losses, label='Validation loss')\r\n",
        "plt.legend(frameon=True)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfOaMLOCPDLo"
      },
      "source": [
        "## Using the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO7YgpfrTeag"
      },
      "source": [
        "data_dir = 'data/dataset/test'\n",
        "test_transforms = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                     ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQuSwHmPt4Xz"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = torch.load('fruitmodel.pth')\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8zqEpIAt7fZ"
      },
      "source": [
        "def predict_image(image):\n",
        "    image_tensor = test_transforms(image).float()\n",
        "    image_tensor = image_tensor.unsqueeze_(0)\n",
        "\n",
        "    input = Variable(image_tensor)\n",
        "    input = input.to(device)\n",
        "    \n",
        "    output = model(input)\n",
        "    index = output.data.cpu().numpy().argmax()\n",
        "    return index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nGJG0X1vlK9"
      },
      "source": [
        "def get_random_images(num):\n",
        "    data = datasets.ImageFolder(data_dir, transform=test_transforms)\n",
        "    classes = data.classes\n",
        "\n",
        "    indices = list(range(len(data)))\n",
        "    np.random.shuffle(indices)\n",
        "    idx = indices[:num]\n",
        "\n",
        "    from torch.utils.data.sampler import SubsetRandomSampler\n",
        "    sampler = SubsetRandomSampler(idx)\n",
        "    loader = torch.utils.data.DataLoader(data, \n",
        "                   sampler=sampler, batch_size=num)\n",
        "    \n",
        "    dataiter = iter(loader)\n",
        "    images, labels = dataiter.next()\n",
        "    return images, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwZI4k--wMsX"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "classes = datasets.ImageFolder(data_dir, transform=test_transforms).classes\n",
        "\n",
        "# convert to PIL img\n",
        "to_pil = transforms.ToPILImage()\n",
        "images, labels = get_random_images(5)\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "\n",
        "for ii in range(len(images)):\n",
        "    image = to_pil(images[ii])\n",
        "    index = predict_image(image)\n",
        "\n",
        "    sub = fig.add_subplot(1, len(images), ii+1)\n",
        "    res = int(labels[ii]) == index\n",
        "\n",
        "    sub.set_title(str(classes[index]) + \":\" + str(res), {'color': 'w'})\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bweMG6_pPZa1"
      },
      "source": [
        "from PIL import Image\r\n",
        "img = Image.open('apple.jpg')\r\n",
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_nDO6oldLtg"
      },
      "source": [
        "print(classes[predict_image(img)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gACo0lW09rnI"
      },
      "source": [
        "## Hosting On Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59JE5M9kUeMF"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\r\n",
        "!unzip -qq ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76yN2O5VxM9a"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjNRuu7gUhOZ"
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 8501 &')\r\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\r\n",
        "  \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZOpwjvqUkiu"
      },
      "source": [
        "!streamlit run app.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXmcFO309Dvg"
      },
      "source": [
        "## Git Management"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dT0W3e17etN"
      },
      "source": [
        "!git config --global user.email \"ongzhizheng@gmail.com\"\r\n",
        "!git config --global user.name \"Fogeinator\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy8j1_bK7H23"
      },
      "source": [
        "!git add app.py width_control.py fruitmodel.pth apple.jpg classified_apple.png loss.png random.png\r\n",
        "!git commit -am \"Add Imgs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z739D09p7pLj"
      },
      "source": [
        "!git push"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOt72RcBY3zI"
      },
      "source": [
        "import streamlit as st\r\n",
        "\r\n",
        "from PIL import Image\r\n",
        "import os, shutil\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy\r\n",
        "\r\n",
        "from width_control import *\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F\r\n",
        "from torchvision import datasets, transforms, models\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "st.set_page_config(\r\n",
        "  page_title=\"FoodRescue\",\r\n",
        "  page_icon=\":seedling:\",\r\n",
        "  layout=\"centered\",\r\n",
        "  initial_sidebar_state=\"collapsed\",\r\n",
        ")\r\n",
        "select_block_container_style()\r\n",
        "\r\n",
        "# -------------------- Header --------------------\r\n",
        "\r\n",
        "st.markdown('# FoodRescue - Rescuing your Food.')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# -------------------- Helper Functions --------------------\r\n",
        "\r\n",
        "data_dir = 'data/dataset/test'\r\n",
        "\r\n",
        "test_transforms = transforms.Compose([transforms.Resize((224, 224)),\r\n",
        "                                      transforms.ToTensor(),\r\n",
        "                                    ])\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "model = torch.load('fruitmodel.pth')\r\n",
        "model.eval()\r\n",
        "\r\n",
        "classes = datasets.ImageFolder(data_dir, transform=test_transforms).classes\r\n",
        "\r\n",
        "@st.cache\r\n",
        "def predict_image(image):\r\n",
        "  image_tensor = test_transforms(image).float()\r\n",
        "  image_tensor = image_tensor.unsqueeze_(0)\r\n",
        "  input = Variable(image_tensor)\r\n",
        "  input = input.to(device)\r\n",
        "  output = model(input)\r\n",
        "  index = output.data.cpu().numpy().argmax()\r\n",
        "  return index\r\n",
        "\r\n",
        "@st.cache\r\n",
        "def get_random_images(num):\r\n",
        "  data = datasets.ImageFolder(data_dir, transform=test_transforms)\r\n",
        "  classes = data.classes\r\n",
        "  indices = list(range(len(data)))\r\n",
        "  np.random.shuffle(indices)\r\n",
        "  idx = indices[:num]\r\n",
        "  from torch.utils.data.sampler import SubsetRandomSampler\r\n",
        "  sampler = SubsetRandomSampler(idx)\r\n",
        "  loader = torch.utils.data.DataLoader(data, \r\n",
        "              sampler=sampler, batch_size=num)\r\n",
        "  dataiter = iter(loader)\r\n",
        "  images, labels = dataiter.next()\r\n",
        "  return images, labels\r\n",
        "\r\n",
        "\r\n",
        "# -------------------- Upload --------------------\r\n",
        "\r\n",
        "st.markdown('<hr>', unsafe_allow_html=True)\r\n",
        "\r\n",
        "st.markdown('## Is your apple/orange/banana spoilt?')\r\n",
        "st.markdown('## üçéüçäüçå')\r\n",
        "\r\n",
        "img = None\r\n",
        "uploaded_file = st.file_uploader('')\r\n",
        "\r\n",
        "if uploaded_file is not None:\r\n",
        "  img = Image.open(uploaded_file)\r\n",
        "  # model only support 3 color channel image\r\n",
        "  if img.mode in (\"RGBA\", \"P\"): img = img.convert(\"RGB\")\r\n",
        "  st.success('Uploaded!')\r\n",
        "\r\n",
        "# -------------------- Display --------------------\r\n",
        "\r\n",
        "img_display = st.empty()\r\n",
        "col_width = False\r\n",
        "\r\n",
        "if img:\r\n",
        "  pred = classes[predict_image(img)]\r\n",
        "  img_display.image(img, use_column_width=col_width)\r\n",
        "  st.markdown('# {}'.format(pred))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBt-Jtj6AJzH"
      },
      "source": [
        ""
      ]
    }
  ]
}